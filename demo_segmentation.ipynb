{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import collections\n",
    "import random  # to test the system by shuffling its segmentations\n",
    "import time  # tacky way to time program's execution time\n",
    "from trie import *\n",
    "from affixes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "def clean_corpus(file):\n",
    "    \"\"\"\n",
    "    file: text file manually edited to feed the trie and perform segmentations\n",
    "    returns clean dataframe. Removed elements by frequency, dropped nans, turned words\n",
    "    to lowercase, dropped duplicates and filtered with the Spanish alphabet\n",
    "    \"\"\"\n",
    "    df =  pd.read_csv(file, delimiter='\\t')\n",
    "    df.dropna(inplace=True)\n",
    "    df = df[df[\"Freq\"] > 600]  # mutates index, hence why .loc is better. okay speed at 100\n",
    "    #  differentce between 90 and 80 not too great but there's 10k rows of more data\n",
    "    # so go with 90. With 2 tries, above 600 is faster. Prefixes are cleaner with 600, though\n",
    "    df.loc[:, \"word\"] = df[\"word\"].str.lower()\n",
    "    df = df[df[\"word\"].apply(spanish_alphabet)]   \n",
    "    df.drop_duplicates(subset=[\"word\"], keep=\"first\", inplace=True)\n",
    "    #df = df[df[\"word\"].str.len() > 1]  # ~df[\"word\"] inverting mask, i.e. doing the opposte\n",
    "    df.reset_index(drop=True, inplace=True)  # resetting index in case it was mutated\n",
    "    return df\n",
    "\n",
    "def spanish_alphabet(word):  \n",
    "    \"\"\"\n",
    "    word: from dataset\n",
    "    returns: boolean value to mask words that can't be built with non-Spanish characters\n",
    "    \"\"\"\n",
    "    alphabet = string.ascii_lowercase \n",
    "    special_chars = \"wxqyz\"\n",
    "    fake_vowels = \"áéíóúyüh\"\n",
    "    sp_alphabet = list(alphabet + \"áéíóúñü\")\n",
    "    flag = True\n",
    "    for i in range(len(word) - 1):\n",
    "        # if current character or next one are not part of the alphabet, mask as False\n",
    "        if (word[i] not in sp_alphabet) or (word[i+1] not in sp_alphabet):\n",
    "            flag = False\n",
    "            break\n",
    "        # if char in uncommon Spanish characters and following character is followed by a \"vowel\"\n",
    "        # then remove. These would not correspond to Spanish words\n",
    "        if (word[i] in special_chars) and (word[i+1] not in fake_vowels):\n",
    "            flag = False\n",
    "            break\n",
    "    # there aren't Spanish words with more than 5 consonants in a row\n",
    "    for i in range(len(word) - 5):\n",
    "        cons = [word[i+j] for j in range(5) if word[i+j] not in fake_vowels + \"aeiou\"]\n",
    "        if len(cons) == 5:  # this is a range of 5 so changing the boolean validation to\n",
    "            # greater than would not be worth it\n",
    "            flag = False\n",
    "    return flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spanish_corpus = clean_corpus(\"clean_spanish.txt\")  \n",
    "print(spanish_corpus)\n",
    "MyTrie = TrieDS()\n",
    "spanish_corpus.apply(lambda row: MyTrie.addWord(row[\"word\"], row[\"Freq\"]), axis=1)  # map may be faster\n",
    "spanish_corpus.apply(lambda row: MyTrie.hasWord(row[\"word\"]), axis=1)\n",
    "spanish_corpus.apply(lambda row: affixBuilder(MyTrie, row[\"word\"]), axis=1)  # don't need to do this twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_suffixes = sort_affixes(rank_affixes(ranked_suffixes))\n",
    "ranked_prefixes = sort_affixes(rank_affixes(ranked_prefixes))\n",
    "top_suffixes = top_affixes(ranked_suffixes, 0.05)\n",
    "top_prefixes = top_affixes(ranked_prefixes, 0.05)  \n",
    "\n",
    "print(len(top_suffixes))\n",
    "print(len(top_prefixes))\n",
    "print(top_suffixes[:10])\n",
    "print(top_prefixes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(prefix_segmentation)\n",
    "random.shuffle(suffix_segmentation)\n",
    "print(prefix_segmentation[:10])\n",
    "print(suffix_segmentation[:10])\n",
    "\n",
    "# %% \n",
    "# this is when the script starts running slower. This can be further optimized\n",
    "def dup_affixes(segmentation):\n",
    "    \"\"\"\n",
    "    segmentation: takes random prefix or suffix segmentation to find words with multiple affixes\n",
    "    returns: list of duplicates\n",
    "    \"\"\"\n",
    "    dup_affix = collections.Counter(\"\".join(word) for word in segmentation)\n",
    "    return [word for word, num in dup_affix.items() if num > 1]\n",
    "\n",
    "dup_prefixes = dup_affixes(prefix_segmentation)\n",
    "dup_suffixes = dup_affixes(suffix_segmentation)\n",
    "# %%\n",
    "def cleaning_affixes(segmentation, dup_affix):\n",
    "    \"\"\"\n",
    "    segmentation: dictionary containing its respective word and segmentation\n",
    "    returns: clean segmentation to include words with multiple and single affixes to facilitate\n",
    "    indexing and reviewing of the trie model\n",
    "    \"\"\"\n",
    "    clean_affix = {}\n",
    "    count = 0\n",
    "    for word in segmentation:\n",
    "        full_word = \"\".join(word)\n",
    "        if full_word in dup_affix:  # checking if word has multiple affixes\n",
    "            if full_word in clean_affix:\n",
    "                clean_affix[full_word].append(f\"{word[0]}-{word[1]}\")\n",
    "            else:\n",
    "                clean_affix.setdefault(full_word, [f\"{word[0]}-{word[1]}\"])\n",
    "        else:  # store its single affix\n",
    "            count += 1\n",
    "            clean_affix.setdefault(full_word, f\"{word[0]}-{word[1]}\")\n",
    "    print(count)  # reviewing how many words have only one affix\n",
    "    return clean_affix\n",
    "\n",
    "clean_prefs = cleaning_affixes(prefix_segmentation, dup_prefixes)\n",
    "clean_suffs = cleaning_affixes(suffix_segmentation, dup_suffixes)\n",
    "\n",
    "#%%\n",
    "print(set(clean_prefs[\"silenciar\"]))\n",
    "print(set(clean_prefs[\"consciente\"]))\n",
    "print(set(clean_suffs[\"esclerosis\"]))\n",
    "print(set(clean_suffs[\"discriminacion\"]))\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Execution time {end - start}\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
